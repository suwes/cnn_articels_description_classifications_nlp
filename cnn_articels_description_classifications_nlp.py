# -*- coding: utf-8 -*-
"""CNN_Articels_Description_Classifications_NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K5i1OCbJCNIpxKJQDiULvhqMhRGp0P6W
"""

from google.colab import files
files.upload()

import os
import zipfile

local_zip = '/content/CNN_Articels_clean.csv.zip'
local_unzip = zipfile.ZipFile(local_zip, 'r')
local_unzip.extractall('/content/')
local_unzip.close()

import pandas as pd
df = pd.read_csv('/content/CNN_Articels_clean.csv')
category = pd.get_dummies(df.Category)
df = pd.concat([df, category], axis=1)
df = df.drop(columns=['Index',
                      'Author',
                      'Date published',
                      'Category',
                      'Section',
                      'Url',
                      'Headline',
                      'Keywords',
                      'Second headline',
                      'Article text'
                      ])
df.tail()

# mengubah dataframe kedalam bentuk array numpy

descriptions = df['Description'].values
categories = df[['business','entertainment','health','news','politics','sport']].values

from sklearn.model_selection import train_test_split
descriptions_train, descriptions_test, categories_train, categories_test = train_test_split(descriptions, categories, test_size=0.2)

# ubah setiap kata pada dataset kita ke dalam bilangan numerik dengan fungsi Tokenizer
import tensorflow as tf
import keras
import keras.preprocessing

from keras.preprocessing import text
from keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer(num_words=8000, oov_token='x')
tokenizer.fit_on_texts(descriptions_train)
tokenizer.fit_on_texts(descriptions_test)


# Setelah tokenisasi selesai, kita perlu membuat mengonversi setiap sampel menjadi sequence
from tensorflow.keras.preprocessing.sequence import pad_sequences
sequence_train = tokenizer.texts_to_sequences(descriptions_train)
sequence_test = tokenizer.texts_to_sequences(descriptions_test)

# menambhakna padding
padding_train = pad_sequences(sequence_train)
padding_test = pad_sequences(sequence_test)
print(padding_train)
len(padding_train)

model = tf.keras.Sequential([
    # arsitektur model kita menggunakan layer Embedding dengan dimensi embedding sebesar 16
    # serta dimensi dari input sebesar nilai num_words pada objek tokenizer. 
    tf.keras.layers.Embedding(input_dim=8000, output_dim=16),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(6, activation='softmax')
])

model.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
    )

model.summary()

# mengaplikasikan callbacks dengan akurasi 85
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.85):
      print("\nAkurasi telah mencapai >85%!")
      self.model.stop_training = True

history = model.fit(
    padding_train,
    categories_train,
    epochs=30, 
    validation_data=(padding_test, categories_test),
    verbose=2
)

log_acc = history.history['accuracy']
log_val_acc = history.history['val_accuracy']
log_loss = history.history['loss']
log_val_loss = history.history['val_loss']

def view_summary(nm,la,mm):
  print(nm, ' : ', float(f'{(mm(la)):.05f}'))

def ln(opt):
  if opt == 1:
    print("_"*50)
  elif opt == 2:
    print('='*50)
  else:
    print('-'*50)

print('\n')
ln(1)
print('SUMMARY'.center(50))
ln(2)
# memanggil fungsi view_summary
view_summary('train accuracy',log_acc,max)
view_summary('validation accuracy',log_val_acc,max)
ln(1)